e-Informatica Software Engineering Journal, Volume 9, Issue 1, 2015, pages: 21-35, DOI 10.5277/e-Inf150102
Cross-Pro ject Defect Prediction With Respect To Code
Ownership Model: An Empirical Study
Marian Jureczko , Lech Madeyski
Institute of Computer Engineering, Control and Robotics, Wroclaw Univeristy of Technology
Faculty of Computer Science and Management, Wroclaw University of Technology
marian.jureczko@pwr.edu.pl, lech.madeyski@pwr.edu.pl
Abstract
The paper presents an analysis of 83 versions of industrial, open-source and academic projects. We
have empirically evaluated whether those project types constitute separate classes of projects with
regard to defect prediction. Statistical tests proved that there exist significant differences between
the models trained on the aforementioned project classes. This work makes the next step towards
cross-project reusability of defect prediction models and facilitates their adoption, which has been
very limited so far.
Keywords: software engineering; defect prediction; empirical study
1. Introduction
Assuring software quality is known to require
time-consuming and expensive development processes.
The costs generated by those processes
may be minimized when the defects are predicted
early on, which is possible by means of defect
prediction models [1]. Those models, based on
software metrics, have been developed by a number
of researchers (see Section 2). The software
metrics which describe artifacts of the software
development process (e.g. software classes, files)
are generally used as the models' input. The
model output usually estimates the probability
of failure, the occurrence of a defect or the expected
number of defects. The predictions are
made for a given artifact. The idea of building
models on the basis of experienced facts, called
inductive inference, is discussed in the context of
software engineering by Samuelis [2].
Defect prediction models are extraordinarily
useful in software testing process. The available
resources are usually limited and, therefore, it
may be difficult to conduct the comprehensive
tests on, and the reviews of, all artifacts. Defect
prediction models are extraordinarily useful in
software testing process. The available resources
are usually limited and, therefore, it may be
difficult to conduct the comprehensive tests on,
and the reviews of, all artifacts [1]. The predictions
may be used to assign varying priorities to
different artifacts (e.g. classes) under test [3, 4].
According to the 80:20 empirical rule, a small
amount of code (often quantified as 20% of the
code) is responsible for the majority of software
defects (often quantified as 80% of the known
defects in the system) [5, 6]. Therefore, it may
be possible to test only a small amount of artifacts
and find a large amount of defects. In short,
a well-designed defect prediction model may save
a lot of testing efforts without decreasing software
quality.
The defect prediction models may give substantial
benefits, but in order to build a model,
a software measurement program must be
launched. According to Kaner and Bond [7], only
few companies establish such programs, even
fewer succeed with them and many of the com22
Marian Jureczko, Lech Madeyski
panies use them only to conform to the criteria which investigate the cross-project defect prediclaid
down in the Capability Maturity Model [8]. tion (i.e. [12,13]) suggest that the code ownership
The costs may be one of the reasons behind the model might be a relevant factor with regard to
limited adoption of the defect prediction mod- the prediction performance.
els, e.g. an average overhead for metrics collec- The rest of this paper is organized as follows:
tion was estimated to be 4-8% of the overall subsequent sections describe related work, the
value [8, 9]. Using cross-project defect prediction design of empirical evaluation used in this study
could reduce the expenditure, since one model (including the details of data collection and analymay
be used in several software projects, which sis methodology), the descriptive statistics of the
means that it is not necessary to launch a com- collected data, the results of empirical evaluation,
pletely new software measurement program for threats to the validity of the empirical study, and
each project. The cross-project defect predic- conclusions.
tion is also helpful with solving the problems
connected with the lack of historical data indispensable
to train a model [10]. Unfortunately, 2. Related Work
the body of knowledge of cross-project defect
prediction does not support us with the results Cross-project reusability of defect prediction
that are advanced enough to be used (details in models would be extremely useful. Such generalthe
next section). The intention of this work is ized prediction models would serve as a starting
to reveal the facts regarding high level predic- point in software development environments that
tion boundaries, particularly the possibility of cannot provide historical data and, as a result,
using prediction models across different project they would facilitate the adoption of defect precode
ownership models. The conducted exper- diction models.
iments aiming at verifying whether cluster of A preliminary work in this area was conducted
projects can be derived from the source code by Subramanyam and Krishnan [14]. The authors
ownership model, where the cluster is a group of investigated a software project where C++, as
software projects that share a common predic- well as Java, were employed and found substantial
tion model. Such finding eases the application differences between classes written in different
of defect prediction by removing the necessity of programming languages with regard to defect
training the model. It is enough to identify the prediction, e.g. the interaction effect (the defect
cluster a given project belongs to and use the count grows with the CBO value for C++ class
common model. but decreases for the Java classes; the relation
This study investigates whether there is a rel- was calculated with respect to the DIT metric).
evant difference between industrial, open-source Hence, the results indicate issues with regard to
and academic software projects with regard to cross-language predictions. The authors investidefect
prediction. In order to explore that po- gated only one project, nevertheless, similar diftential
disparity, the data from 24 versions of 5 ficulties might arise in cross-project predictions.
industrial projects, 42 versions of 13 open-source Nagappan et al. [15] analyzed whether defect
projects and 17 versions of 17 academic projects predictors obtained from one project history are
were collected. Several defect prediction models applicable to other projects. It turned out that
were built and the efficiency of the predictions there was no single set of metrics that would
was compared. Statistical methods were used in fit into all five investigated projects. The defect
order to decide whether the obtained differences prediction models, however, could be accurate
were significant or not. It is worth mentioning when obtained from similar projects (the similarthat
a somehow related question of suitability of ity was not precisely defined, though). The study
software quality model for projects within differ- was extended by Zimmerman et al. [12], who
ent application domains was raised by Villalba performed 622 cross-project predictions for 12
et al. [11], whereas the results of other works real world applications. A project was considered
Cross-Project Defect Prediction With Respect To Code Ownership Model: An Empirical Study
23
as a strong predictor for another project when
all precision, recall, and accuracy were greater
than 0.75. Only 21 cross-project validations satisfied
this criterion, which sets the success rate
at 3.4%. Subsequently, the guidelines to assess
the cross-project prediction chance of success
were given. The guidelines were summarized in
a decision tree. The authors constructed separate
trees for assessing prediction precision, recall, and
accuracy, but only the tree for precision was given
in the paper.
A study of cross-company defect prediction
was conducted by Turhan et al. [10]. The authors
concluded that there is no single set of static
code features (metrics) that may serve as defect
predictor for all the software projects. The effectiveness
of the defect prediction models was
measured using probability of detection (pd) and
probability of false alarm (pf). Cross-company
defect prediction dramatically increased pd as
well as pf. The authors were also able to decrease
the pf by applying the nearest neighbor filtering.
The similarity measure was the Euclidean distance
between the static code features. However,
there was still a drawback for cross-company defect
prediction models: the project features which
might influence the effectiveness of cross-company
predictions were not identified.
In an earlier paper [13], we presented an empirical
study showing that data mining techniques
may be used to identify project clusters
with regard to cross-project defect prediction.
The k-means and Kohonen's neural networks
were applied to correlation vectors in order
to identify the clusters. The correlation vectors
were calculated for each version of each
project respectively and represented Pearson's
correlation coefficients between software metrics
and numbers of defects. Subsequently, a defect
prediction model was created for each identified
cluster. In order to validate the existence
of a cluster, the efficiency of the cluster model
was compared with the efficiency of a general
model. The general model was trained using
data from all the projects. Six different clusters
were identified and the existence of two of
them was statistically proven. The clusters characteristics
were consistent with Zimmerman's
findings [12] about the factors that are critical
in cross-project prediction. In our paper, we
make a step towards simplifying the setup of
defect prediction in the software development
process. The laborious activities regarding calculation
of correlation vectors and mining the
clusters are not needed as it is obvious from
the very beginning to which cluster a project
belongs. A subset of the same data set had already
been analysed in [16,17]. In [16] 5 industrial
and 11 open-source projects were investigated.
The study was focused on the role of the size
factor in defect prediction. The other paper was
focused on the cross-project defect prediction.
In [17], published in Polish, being a preliminary
study to this one, we focused on the differences
and similarities between industrial, open-source
and academic projects, whereas in this paper, we
additionally performed a comprehensive statistical
analysis. As a result, new projects may take
advantage of the prediction models developed
for the aforementioned classes of the existing
projects.
It is also worth mentioning that a different approach,
based on the idea of inclusion additional
software projects during the training process, can
provide a cross-project perspective on software
quality modelling and prediction [18].
A comprehensive study of cross-project defect
prediction was conducted by He et al. [19]. The
authors investigated 10 open source projects to
check whether training data from other projects
can provide better prediction results than training
data from the same project - in the best cases
it was possible. Furthermore, in 18 out of 34 cases
the authors were able to obtain a Recall greater
than 70% and a Precision greater than 50% for
the cross-project defect prediction.
3. Empirical Evaluation Design
3.1. Data Collection
The data from 83 versions of 35 projects was collected
and analyzed. It covers 24 versions of 5 industrial
projects, 42 versions of 13 open-source
projects and 17 versions of 17 academic projects.
24
Marian Jureczko, Lech Madeyski
The number of versions is greater than the num- - Chidamber & Kemerer metrics suite [20]:
ber of projects because there were projects in Weighted Method per Class (WMC), Depth of
which data from several versions were collected. Inheritance Tree (DIT), Number Of Children
For example, in the case of Apache Ant project (NOC), Coupling Between Object classes
(http://ant.apache.org), versions 1.3, 1.4, 1.5, 1.6 (CBO), Response For a Class (RFC) and Lack
and 1.7 were analyzed. For each of the analysed of Cohesion in Methods (LCOM);
versions there was an external release, which was - a metric suggested by Henderson-Sellers [21]:
visible to the customer or user. Lack of Cohesion in Methods (LCOM3);
Each of the investigated industrial projects - Martin's metrics [22]: Afferent Couplings (Ca)
is a custom-built enterprise solution. All of the and Efferent Couplings (Ce).
industrial projects have been already success- - QMOOD metrics suite [23]: Number of Public
fully developed by different teams of 10 to 40 Methods (NPM), Data Access Metric (DAM),
developers and installed in the customer envi- Measure Of Aggregation (MOA), Measure of
ronments. All of them belong to the insurance Functional Abstraction (MFA) and Cohesion
domain but implement different feature sets on Among Methods (CAM);
top of Java-based frameworks. Each of the in- - quality oriented extension of Chidamber
dustrial projects were developed by the same & Kemerer metrics suite [24]: Inheritance
vendor. Coupling (IC), Coupling Between Methods
The following open-source projects were in- (CBM) and Average Method Complexity
vestigated: Apache Ant, Apache Camel, Apache (AMC),
Forrest, Apache Log4j, Apache Lucene, Apache - two metrics, that are based on the McCabe's
POI, Apache Synapse, Apache Tomcat, Apache cyclomatic complexity measure [25]: MaxiVelocity,
Apache Xalan, Apache Xerces, Ckjm mal Cyclomatic Complexity (Max_CC) and
and Pbeans. Average Cyclomatic Complexity (Avg_CC),
The academic projects were developed by the - Lines Of Code (LOC),
fourth and the fifth-year graduate MSc computer - Defects - the dependent variable; in the case
science students. The students were divided into of industrial and open source projects the
the groups of 3, 4 or 5 persons. Each group de- sources of the defect data were testers and
veloped exactly one project. The development end users, in the case of academic projects
process was highly iterative (feature driven de- the source of the defect data were students
velopment). Each project lasted one year. During (not involved in a development of a particthe
development for each feature UML documen- ular projects) who validated the developed
tation was prepared. Furthermore, high level of software against the specification during the
test code coverage was obtained by using lat- last month (devoted to testing) of the 1-year
est testing tools, e.g. JUnit for unit tests, Fit- project.
Nesse for functional tests. The last month of Definitions of the metrics listed above can
development was used for additional quality as- be found in [16]. In order to collect the metsurance
and bug fixing; the quality assurance rics, we used a tool called Ckjm (http://gromit.
was conducted by external group of subjects (i.e. iiar.pwr.wroc.pl/p_inf/ckjm). The version of
not the subjects involved in the development). Ckjm employed here was reported earlier by
The projects were from different domains, were Jureczko and Spinellis [16]). The defect count
built using different frameworks, had different was collected with a tool called BugInfo (http:
architectures and covered different sets of func- //kenai.com/projects/buginfo). The collected
tionalities, nonetheless all of them were written metrics are available online in a Metric Repository
in Java. (all metrics: http://purl.org/MarianJureczko/
The objects of the measurement were soft- MetricsRepo, metrics used in this research:
ware development products (Java classes). The http://purl.org/MarianJureczko/MetricsRepo/
following software metrics were used in the study: IET_CrossProjectPrediction). Data sets related
Cross-Project Defect Prediction With Respect To Code Ownership Model: An Empirical Study
25
to the analyzed software defect prediction models order of predicted defects according to the model
are available from the R package [26] to streamline M , and let d1; d2; :::; dn be the number of dereproducible
research [27, 28]. fects in each class. Di is the P(d1; :::; di) , i.e.,
The employed metrics might be considered the total defects in the first i classes. Let k be
as the classic ones. Each of them is in use for at the smallest index so that Dk > 0:8 Dn, then
least several years. Hence, the metrics are well E(M; v) = k=n 100%. Such evaluation funcknown,
already recognized by the industry and tion has been used as it clearly corresponds with
have a good tool support. There is a number of the software projects reality we faced with. It is
other metrics, some of them very promising in closely related to a quality goal: detect at least
the field of defect prediction, e.g. the cognitive, 80% of defects. The evaluation function shows
the dynamic and the historical metrics [15,29,30]. how many classes must be tested (in practice it
A promising set of metrics are process metrics corresponds well to how much effort must be comanalysed
by Madeyski and Jureczko [31]. Some mitted) to reach the goal when testing according
of them, like Number of Distinct Committers to prediction model output. The properties of
(NDC) or Number of Modified Lines (NML), can the group of evaluation functions that the one
significantly improve defect prediction models selected by us belongs to have been analysed by
based on classic software product metrics [31]. Weyuker et al. [33].
Nevertheless, we decided not to use them, since The empirical evaluation is defined in
those metrics are not so popular yet (especially in a generic way which embraces three investigated
the industry) and the collecting process could be classes of software projects. Let A; B and C be
challenging. One may expect that the limited tool those classes (i.e. industrial, open-source and acasupport
would result in decreasing the number of demic, respectively). Let us interpret the classes
investigated projects, which is a crucial factor in of software projects as the sets of versions of
cross-project defect prediction. We fully under- software projects. Let A be the object of the
stand and accept the value of using metrics that current empirical evaluation. B and C will be
describe a variety of features. Furthermore, we investigated in subsequent experiments using the
are involved in the development of a tool which analogous procedure. Let a be a member of the
includes support for the historical metrics [32]. set A, a 2 A. Let Mx be the defect prediction
We are going to use the tool to collect metrics model which was trained using data from verfor
future experiments. sions that belong to set X. Specifically, there are
MA, MB, MC and MB[C (B \ C = :A). Subse3.2.
Data Analysis Methodology quently, the following values were calculated for
each a 2 A:
The empirical evaluation described further was - E(MA; a) - let's call the set of obtained values
performed to verify whether there is a difference EA,
between industrial, open-source and academic - E(MB; a) - let's call the set of obtained values
projects with regard to defect prediction. Sta- EB,
tistical hypotheses were formulated. The defect - E(MC ; a) - lets call the set of obtained values
prediction models were built and applied to the EC ,
investigated projects. The efficiency of prediction - E(MB[C ; a) - let's call the set of obtained
was used to evaluate the models and to verify values EB[C .
the hypotheses. The following statistical hypothesis may be
To render that in a formal way, it is neces- formulated with the use of EA, EB, EC and EB[C
sary to assume that E(M; v) is the evaluation sets:
function. The function assesses the efficiency of - H0;A;B - EA and EB come from the same
prediction of the model M on the version v of distribution.
the investigated software project. Let c1; c2; :::; cn - H0;A;C - EA and EC come from the same
be the classes from the version v in descending distribution.
26
Marian Jureczko, Lech Madeyski
Table 1. Descriptive statistics of metrics in different projects classes (X - mean; s - standard
deviation; r - Pearson correlation coefficient; - correlation significant at 0,05 level)
WMC
DIT
NOC
CBO
RFC
LCOM
Ca
Ce
NPM
LCOM3
LOC
DAM
MOA
MFA
CAM
IC
CBM
AMC
Max_cc
Avg_cc
Defects
X
5:2
3
0:6
15:1
24:7
37:6
2:8
12:3
3:4
1:4
170:4
0:2
0:1
0:6
0:6
1:1
1:7
30:4
3:3
1:3
0:232
Industrial
s
8:6
1:7
8:8
20:8
27:5
660:2
17:7
10:3
7:9
0:6
366:7
0:3
1
0:4
0:2
1:1
2:4
39:8
5:7
1:5
0:887
r
0:13
0:07
0
0:26
0:28
0:03
0:16
0:24
0:08
0:04
0:26
0:02
0:03
0:06
0:13
0:04
0:03
0:14
0:17
0:13
X
10:5
2:1
0:5
10:3
27:1
95:2
5:1
5:4
8:4
1:1
281:3
0:5
0:8
0:4
0:5
0:5
1:5
28:1
3:8
1:3
0:738
Open-source
s
13:7
1:3
3:4
16:9
33:3
532:9
15:3
7:4
11:5
0:7
614:7
0:5
1:8
0:4
0:3
0:8
3:1
80:7
7:5
1:1
1:906
r
0:29
0:01
0:03
0:20
0:34
0:19
0:11
0:26
0:22
0:07
0:29
0:06
0:27
0:02
0:19
0:06
0:10
0:07
0:17
0:12
X
9:7
2:1
0:2
8
26
62
3:8
4:7
7:4
1:1
248:1
0:6
0:8
0:3
0:5
0:3
0:5
21:2
3:2
1:1
0:382
Academic
s
10:8
1:7
1:5
8:1
30:6
276:5
6:7
5:9
8:7
0:6
463:4
0:5
1:6
0:4
0:2
0:5
1:5
24:7
5:5
0:8
1:013
r
0:38
0:29
0:04
0:25
0:53
0:37
0:19
0:38
0:08
0:11
0:53
0:07
0:27
0:16
0:17
0:03
0:02
0:24
0:31
0:17
- H0;A;B[C - EA and EB[C come from the
same distribution.
The alternative hypothesis:
- H1;A;B - EA and EB come from different distributions.
- H1;A;C - EA and EC come from different distributions.
- H1;A;B[C - EA and EB[C come from different
distributions.
When the alternative hypothesis is accepted
and mean(EA) < mean(EX ), there is a significant
difference in prediction accuracy: the model
trained on the data from set A gives a significantly
better prediction than the model trained
on the data from set X . The predictions are made
for all the project versions which belong to the
set A. Hence, the data from set X should not
be used to build defect prediction models for the
project versions which belong to set A.
The hypotheses were evaluated by the parametric
t-test for dependent samples. The general
assumptions of parametric tests were investigated
beforehand. The homogeneity of variance was
tested using Levene's test and the normality of
distribution was tested using the Shapiro-Wilk
test [34]. All hypotheses were tested on the default
significance level: = 0:05.
4. Experiments and Results
4.1. Descriptive Statistics
The three aforementioned classes of software
projects, namely: industrial, open-source and
academic, were described in Table 1. The description
provides information about the mean value
and standard deviation of each of the analyzed
software metrics. Each metric is calculated per
Java class, and the statistics are based on all Java
classes in all projets that belong to a given class
of projects. Moreover, the correlations with the
number of defects were calculated and presented.
Most of the size related metrics (WMC, LCOM,
LOC, Max_cc and Avg_cc) had higher values
in open-source projects, while coupling metrics
(CBO and Ce) had the greatest values in the
industrial projects. In the case of eachof the inCross-Project
Defect Prediction With Respect To Code Ownership Model: An Empirical Study
27
Table 2. The number of classes per project
Industrial
X s
Open-source
X s
Academic
X s
No of classes 2806:3
831:7
302:6
219:4
56:4
58:4
Table 3. The number of defects
per Java class
Type
Open-source
Industrial
Academic
X
:7384
:2323
:3816
s
1:9
:9
1:0
vestigated project classes, the RFC metric has a
higher correlation coefficient with the number of
defects. However, there are major differences in
its value (it is 0.28 in the case of the industrial
projects, and 0.53 in the case of the academic
projects) and in the sets of other metrics that
are highly correlated with the number of defects.
Collected data suggest that in all kinds of
the projects (industrial, open-source and academic)
the mean LOC per class follows the rule
of thumb presented by Kan in his book [35] (Table
12.2 in Chapter 12) that LOC per C++ class
should be less than 480. The similar value is
expected for Java. Bigger classes would suggest
poor object-oriented design. Low LOC per class
does not mean that the code under examination
is small. To give an impression with regard to the
size of the investigated projects Table 2 presents
numbers of classes per project.
The number of defects per Java class is presented
in Table 3. It appears that the number of
defects per Java class in industrial and academic
projects is close with regards to standard deviation
and mean, while open source projects are
characterized by higher standard deviation as well
as mean. The plausible explanation is that lower
standard deviations in academic and industrial
projects come from more homogeneous development
environment than in open source projects.
4.2. Empirical Evaluation Results
The empirical evaluation has been conducted
three times. Each time a different class of software
projects was used as the object of study.
4.2.1. Industrial Projects
The descriptive statistics are summarized in Table
4. The models that were trained on the data
from the industrial projects ('Industrial models'
in Table 4) gave the most accurate prediction.
The predictions were made only for the industrial
projects. Furthermore, the mean value of the
evaluation function E(M; v) was equal to 50.82
in the case of the 'Industrial models'. In the case
of the other models, the mean values equaled
53.96, 55.38 and 73.59. A smaller value of the
evaluation function implies better predictions.
The predictions obtained from the models
trained on the data from the industrial projects
were compared with the predictions from the
other models. The predictions were made only for
the industrial projects. The difference was statistically
significant only in the case of comparison
with the predictions from models trained on the
data from the academic projects (see Table 5).
In this case the calculated effect size d = 1:56
is extremely high (according to magnitude labels
proposed by Cohen d effect size equal to
:2 is considered small, equal to :5 is considered
medium, while equal to :8 is considered high)
while the power of a test (i.e., the probability
of rejecting H0 when in fact it is false) is equal
to 1. This important finding shows that there
exist significant differences between industrial
and academic software projects with respect to
defect prediction.
4.2.2. Open-source Projects
The descriptive statistics of the results of applying
defect prediction models to open-source
projects are presented in Table 6. The models,
which were trained on the data from the
open-source projects ('Open-source models' in
Table 6), gave the most accurate prediction.
Table 7 shows the t-test statistics, power
and effect size calculations for the open-source
projects.
In all of the cases p-value is lower than :05.
However, instead of coming to the conclusions
28
Marian Jureczko, Lech Madeyski
Table 4. Models evaluations for the industrial projects
Model
Non-industrial
Open-source
Academic Industrial
X
s
53:96
13:29
55:38
12:01
73:59
9:68
50:82
9:86
Table 5. Dependent samples t-test for the industrial
projects.
H0;ind;open[acad
H0;ind;open
H0;ind;acad
Table 6. Models evaluations for the open-source projects.
Model Industrial
Non-open-source
Academic
Open-source
t; df = 23
p
effect size d
power
X
s
57:67
19:22
:979
:338
:200
:244
57:26
18:02
now we suggest to perform the Bonferroni correction
(explained in Section 4.2.4) beforehand.
Calculated effect sizes are between medium and
small int the first two cases, while medium to
large in the last case which is an important finding
suggesting serious differences between open
source and academic projects with respect to
defect prediction. The power of a test is calculated
as well suggesting very high (close to 1)
probability of rejecting H0 when in fact it is
false.
4.2.3. Academic Projects
The descriptive statistics of the results of applying
defect prediction models to academic projects
are presented in Table 8. The obtained results are
surprising. The 'Academic models' gave almost
the worst predictions. Slightly worse were only
the 'Industrial models', whilst the 'Open-source
models' and 'Non-academic models' gave definitely
better predictions.
The analysis presented in Table 9 is based on
the t-test for dependent samples. The predictions
obtained from the models that were trained on
the data from the academic projects were compared
with predictions from the other models.
The predictions were made only for the academic
1:482
:152
:302
:418
7:637
:000
1:559
1
65:17
14:31
54:00
16:66
projects. The differences were not statistically
significant, the effect sizes was below small (in
the first and third case) and between small and
medium in the second case.
4.2.4. Bonferroni Correction
Since several different hypotheses were tested
on the same data set, the following kinds of
errors were likely to occur: errors in inference,
including confidence intervals which fail to include
their corresponding population parameters,
or hypothesis tests that incorrectly reject
the null hypothesis. Among several statistical
techniques that have been developed to prevent
such instances is the Bonferroni correction.
The correction is based on the idea that
if n dependent or independent hypotheses are
being tested on a data set, then one way of
maintaining the familywise error rate is to test
each individual hypothesis at a statistical significance
level of 1=n times. In our case n = 9
as there are nine different hypotheses. Hence,
the significance level should be decreased to
0:05=9 = 0:0055. Consequently, the H0;ind;acad
and H0;open;acad hypotheses will be rejected but
H0;open;ind and H0;open;ind[acad hypotheses will
not be rejected.
t; df = 41
p
effect size d
power
t; df = 16
p
effect size d
power
2:363
:023
:365
:752
0:312
:759
:076
:009
Table 8. Models evaluations for the academic projects.
Model Industrial
Open-source
Non-academic
Academic
X
s
56:34
20:71
50:60
15:56
53:19
18:54
55:02
20:21
Table 9. Dependent samples t-test for the academic projects.
H0;acad;ind
H0;acad;open
H0;acad;ind[open
Cross-Project Defect Prediction With Respect To Code Ownership Model: An Empirical Study
29
Table 7. Dependent samples t-test for the open-source
projects.
H0;open;ind
H0;open;ind[acad
H0;open;acad
4.3. Which Metrics are Relevant?
There are some evidences that the three investigated
code ownership models differs with respect
to defect prediction. Therefore, it could be helpful
for further research to identify the casual relations
that drive those differences. Unfortunately, the
scope of software metrics (independent variables
of the defect prediction models) does not cover
many aspects that may be the direct cause of
defects. Let us assume that there is an inexperienced
developer that gets a requirement to implement
and he does something wrong, he introduces
a defect into the system. The true cause of the
defect is a composition of several factors including
the developer experience, requirement complexity
and the level of maintainability of the parts of
the system that were changed by the developer.
Only a fraction of the aforementioned factors can
be covered by the metrics available from software
repositories and hence many of them must be
ignored by the defect prediction models. Taking
into consideration the above arguments we decided
not to define the casual relations upfront,
but investigate what emerges from the models we
obtained. Since it does not follow the commonly
2:193
:034
:338
:695
4:325
:000
:667
:995
1:484
:157
:360
:412
0:696
:496
:169
:164
used procedure (start with a theory and then
look for confirmation in empirical data), it is
important to keep in mind that results of such
analysis does not indicate casual relations, but
only coexistence of some phenomena.
The analysis is based on the relevancy of
particular metrics in models obtained for different
code ownership types. The defect prediction
model has the following form:
ExpectedNumberOfDefects = a1 M1 +a2 M2 : : :
where ai represents coefficients obtained from regression,
Mi software metrics (independent variables
in the prediction). For each metric we calculated
its importance factor (the factors are
calculated for each code ownership model respectively)
using the following form:
IFMi = X jaj
j
ai
Mi
Mj j
where Mi is the average value of metric Mi in the
type of code ownership for which the factor is calculated
(the averages are reported in Tab. 1). The
above definition results in a factor that shows for
30
Marian Jureczko, Lech Madeyski
Model
WMC
DIT
NOC
CBO
RFC
LCOM
Ca
Ce
NPM
LCOM3
LOC
DAM
MOA
MFA
CAM
IC
CBM
AMC
Max_cc
Avg_cc
Industrial
0:06
0:08
0:01
0:17
0:19
0
0
0
0
0:13
0:05
0:01
0
0:05
0:16
0
0:03
0:04
0:01
0
Open-source
0
0
0
0
0:35
0
0:05
0:17
0
0
0:14
0:17
0:12
0
0
0
0
0
0
0
Academic
0
0:08
0
0:36
0:04
0
0:17
0:20
0:01
0:05
0
0:04
0
0:02
0
0:02
0:01
0
0
0
Table 10. Importance of metrics in different code ownership models.
given metric what part of the prediction model 5. Threats to Validity
output is driven by the metric and additionally
preserves the sign of the metric's contribution. 5.1. Construct Validity
The obtained values of importance factors are
reported in Tab. 10. Threats to construct validity refer to the extent
The importance factors show that there are to which the measures accurately reflect the theosimilarities
as well as differences between the retical concepts they are intended to measure [34].
prediction models trained for different types of The mono-method bias reflects the risk of a sincode
ownership. In all of them an important role gle means of recording measures. As a result, an
is played by the RFC metric and all of them important construct validity threat is that we
take into consideration coupling related metrics. cannot guarantee that all the links between bugs
However, in the case of academic projects these and versioning system files, and, subsequently,
are Ca and Ce, in the case of open-source Ce classes, are retrieved (e.g. when there is no bug
is much more important than Ca and for in- reference identifier in the commit comment), as
dustrial projects only CBO matters. There are bugs are identified according to the comments in
also metrics with positive contribution in only the source code version control system. In fact,
one type of projects (positive contribution means this is a widely known problem and the method
that the metric value grows with the number that we adopted is not only broadly used but
of expected defects), i.e. LCOM3 for industrial also represents the state of the art with respect
projects (as well as the mentioned earlier CBO), to linking bugs to versioning system files and
MOA and LOC for the open-source projects, classes [36, 37].
DIT for the academic ones. More significant dif- A closely related threat concerns anonymous
ferences have been observed with regard to neg- inner classes. We cannot distinguish whether
ative contribution. The greatest negative con- a bug is related to anonymous inner classes or
tribution for academic projects have CBO and their containing class, due to the file-based nature
LCOM3, for open-source DAM and for indus- of source code version control systems. Hence, it
trial CAM. is a common practice not to take into consideration
the inner classes [38-40]. Fortunately, the
inner classes usually constitute a small portion
of all classes (in our study it was 8.84%).
Furthermore, the guidelines of commenting
bugfixes may vary among different projects.
Therefore, it is possible that the interpretation
of the term bug is not unique among the investigated
projects. Antoniol et al. [41] showed that
a fraction of issues marked as bugs are problems
unrelated to corrective maintenance. We did our
best to remove such occurrences manually but in
future research we plan to apply the suggestion
by Antoniol et al. to filter the non-bug issues out.
It is also worth mentioning that it was not
possible to track operations like changing the
class name or moving the class between packages.
Therefore, after such a change, the class is
interpreted as a new one.
5.2. Statistical Conclusion Validity
Threats to statistical conclusion validity relate to
the issues that affect the validity of inferences. In
our study we used robust statistical tools: SPSS
and Statistica.
5.3. Internal Validity
terprise solutions that employ database systems.
Furthermore, all the industrial projects were developed
by the same vendor which poses a major
threat to external validity. In consequence, it is
possible to define an alternative hypothesis that
explains the differences between clusters, e.g. the
open-source cluster can be redefined into tools
& libraries, while the industrial cluster can be
redefined into enterprise database oriented solution
and then a hypothesis that regards difference
between such clusters may be formulated. Unfortunately,
those alternative hypotheses cannot
be invalidated without additional projects and
even with additional projects we cannot avoid
further alternative hypotheses with more fancy
definitions of cluster boundaries. The root cause
of this issue is the sample selection procedure
which does not guarantee random selection. Only
small part of the population of software projects
is available for researchers and collecting data
for an experiment is a huge challenge taking
into account how difficult is to get access to the
source code or software metrics of real, industrial
projects. We were addressing this issue by taking
into consideration the greatest possible number
of projects we were able to cover with a common
set of software metrics. It does not solve the
issue, but reduces the risk of accepting wrong
hypothesis due to some data constellations that
are a consequence of sample selection.
Cross-Project Defect Prediction With Respect To Code Ownership Model: An Empirical Study
31
The threats to internal validity concern the true
causes (e.g., external factors) that may affect
the outcomes observed in the study. The external
factor we are aware of is the human factor 5.4. External Validity
pointed out by D'Ambros et al. [38]. D'Ambros
et al. decided to limit the human factor as far as The threats to external validity refer to the genpossible
and chose not to consider bug severity eralization of research findings. Fortunately, in
as a weight factor when evaluating the number our study we considered a wide range of different
of defects. We decided to follow this approach, as kinds of projects. They represent different ownOstrand
et al. reported how those severity ratings ership models (industrial, open source and acaare
highly subjective and inaccurate [42]. demic), belong to different application domains
Unfortunatley, each of the investigated clus- and have been developed according to different
ters (i.e. code ownership models) has limited vari- software development processes. However, our seability.
All academic projects were developed at lection of projects is by no means representative
the same university. However, they differ a lot and that poses a major threat to external validity.
with respect to requirements and architecture. For example, we only considered software projects
Only two open-source projects (PBeans and developed in Java programming language. Forckjm)
do not come from Apache and all of them tunately, thanks to this limitation all the code
can be classified as a tool or library what is in metrics are defined identically for each system,
opposition to industrial projects which are en- so we have alleviated the parsing bias.
32
6. Conclusions
Our study has compared three classes of software
projects (industrial, open-source and academic)
with regard to defect prediction. The
analysis comprised the data collected from 24
versions of 5 industrial projects, 42 versions of
13 open-source projects, and 17 versions of 17
academic projects. In order to identify differences
among the classes of software projects listed
above, defect prediction models were created and
applied. Each of the software project classes was
investigated through verifying three statistical hypotheses.
The following two noteworthy findings
were identified: two of the investigated hypotheses
were rejected: H0;ind;acad and H0;open;acad. In the
case of H0;open;ind[acad and H0;open;ind p-values
were below :05, but the hypotheses can not be
rejected due to the Bonferroni correction. Such
results are not conclusive due to threats to external
validity discussed in Secton 5.4, as well
as the possibility that even small changes in the
input data may change the decision regarding
hypothesis rejection in both directions and thus
we encourage further investigation.
As a result, we obtained some evidence
that the open-source, industrial and academic
projects may be treated as separate classes of
projects with regard to defects prediction. In
consequence, we recommend against using models
trained on the projects from different code
ownership model, e.g. making predictions for
an industrial project with a model trained on
academic or open-source projects. Of course the
investigated classes (i.e. academic, industrial and
open-source) may not be optimal and smaller
classes could be identified in order to increase the
prediction performance. Identification of smaller
projects classes constitutes a promising direction
for further research.
The prediction models trained for each of the
investigated classes of projects were further analysed
in order to reveal key differences between
them. Let us focus on the differences between
open-sources and industrial ones as we have very
limited evidences to support the thesis that academic
projects constitutes a solid class of projects
with respect to defect prediction. However the
Marian Jureczko, Lech Madeyski
analysis revealed an almost self-explaining fact
with regard to this class of projects. Namely,
the deeper the inheritance tree the more likely
a student will introduce a defect in such a class.
Typically, API of a class with a number of ancestors
is spread among the parent classes and
thus a developer that looks at only some of them
may not understand the overall concept and introduce
changes that are in conflict with the
source code of one of the other classes. In consequence,
we may expect that inexperienced developer,
e.g. a student, will miss some important
details.
The differences between open-source and industrial
projects can be explained by the so called
crowd-driven software development model commonly
used in open-source projects. High value
of the model output in those projects is mainly
driven by the LOC and MOA metrics. First of
them simply represents size of a class and it is
not surprising that it could be challenging to
understand a big class for someone who commits
to a project only occasionally. Furthermore, the
MOA metric can be considered in terms of number
of classes that must be known and understood
to effectively work with a given one, which creates
additional challenges for developers that do not
work in a project in a daily manner. There also
is the negative contribution of the DAM metrics
which also fits well to the picture as high values of
this metric correspond with low number of public
attributes and thus narrows the scope of source
code that a developer should be familiar with.
Both, industrial and open-source models use the
RFC metric, however, in the case of open-source
its more relevant which also supports the aforementioned
hypothesis regarding crowd-driven development.
The industrial projects are usually
developed by people who know the project under
development very well. That does not mean
that everyone know everything about each class.
When it is necessary to be familiar with a number
of different classes to make a single change in
the project it is still likely to introduce a defect.
However, in the case of industrial projects the
effect is not so strong. Furthermore, the industrial
prediction model uses metrics that regard flaws
in the class internal structure, i.e. LCOM3 and
Cross-Project Defect Prediction With Respect To Code Ownership Model: An Empirical Study
33
CAM), which make challenges in the development [4] M. L. Hutcheson and L. Marnie, Software testing
regardless of developer knowledge about other fundamentals. John Wiley & Sons, 2003.
classes. [5] B. W. Boehm, “Understanding and controlling
The models that were trained on the academic software costs,” Journal of Parametrics, Vol. 8,
projects gave usually the worst predictions. Even [6] NGo..D1e,n1a9r8o8a,npdp.M3.2-P6e8z.zè, “An empirical evaluain
the case of making predictions for academic tion of fault-proneness models,” in Proceedings
projects, the models trained on the academic of the 24rd International Conference on Software
projects did not perform well. It was not the pri- Engineering, 2002. ICSE 2002. IEEE, 2002, pp.
mary goal of the study, but the obtained results 241-251.
made it possible to arrive at that newsworthy [7] C. Kaner and W. P. Bond, “Software engineering
conclusion. The academic projects are not good metrics: What do they measure and how do we
as a training set for the defect prediction models. know?” in 10th International Software Metrics
Symposium. IEEE, 2004, p. 6.
Probably, they are too immature and thus have [8] N. E. Fenton and M. Neil, “Software metrics:
too chaotic structure. The obtained results point successes, failures and new directions,” Journal
to the need of reconsidering the relevancy of the of Systems and Software, Vol. 47, No. 2, 1999,
studies on defect prediction that rely solely on the pp. 149-157.
data from the academic projects. Academic data [9] T. Hall and N. Fenton, “Implementing effecsets
were used even in the frequently cited [43-45] tive software metrics programs,” IEEE Software,
and recently conducted [46, 47] studies. Vol. 14, No. 2, 1997, pp. 55-65.
Detailed data are available via a web-based [10] JB.. DTuirhSatnef,anTo., M“Oennzietsh,e Ar.elaBt.iveBenvaerlu,eanodf
metrics repository established by the authors cross-company and within-company data for de(http://purl.org/MarianJureczko/MetricsRepo).
fect prediction,” Empirical Software Engineering,
The collected data may be used by other re- Vol. 14, No. 5, 2009, pp. 540-578.
searchers for replicating presented here experi- [11] M. T. Villalba, L. Fernández-Sanz, and
ments as well as conducting own empirical studies. J. Martínez, “Empirical support for the generThe
obtained defect prediction models related ation of domain-oriented quality models,” IET
to the conducted empirical study presented in software, Vol. 4, No. 1, 2010, pp. 1-14.
[12] T. Zimmermann, N. Nagappan, H. Gall, E. Giger,
this paper are available online (http://purl.org/ and B. Murphy, “Cross-project defect prediction:
MarianJureczko/IET_CrossProjectPrediction). a large scale experiment on data vs. domain vs.
However, we recommend using the models for process,” in Proceedings of the the 7th joint meetcross-project
defect prediction with great cau- ing of the European software engineering confertion,
since the obtained prediction performance ence and the ACM SIGSOFT symposium on The
is moderate and presumable in most cases can foundations of software engineering. ACM, 2009,
be surpassed by a project specific model. pp. 91-100.
[13] M. Jureczko and L. Madeyski, “Towards identifying
software project clusters with regard to
References defect prediction,” in Proceedings of the 6th International
Conference on Predictive Models in
Software Engineering. ACM, 2010, p. 9.
[14] R. Subramanyam and M. S. Krishnan, IEEE
Transactions on Software Engineering, Vol. 29,
No. 4, 2003, pp. 297-310.
[15] N. Nagappan, T. Ball, and A. Zeller, “Mining
metrics to predict component failures,” in Proceedings
of the 28th international conference on
Software engineering. ACM, 2006, pp. 452-461.
[16] M. Jureczko and D. Spinellis, “Using
object-oriented design metrics to predict
software defects,” in Models and Methods of
System Dependability. Oficyna Wydawnicza
[1] L. Briand, W. Melo, and J. Wust, “Assessing the
applicability of fault-proneness models across
object-oriented software projects,” IEEE Transactions
on Software Engineering, Vol. 28, No. 7,
2002, pp. 706-720.
[2] L. Samuelis, “On principles of software
engineering-role of the inductive inference,”
e-Informatica Software Engineering Journal,
Vol. 6, No. 1, 2012, pp. 71-77.
[3] L. Fernandez, P. J. Lara, and J. J. Cuadrado,
“Efficient software quality assurance approaches
oriented to UML models in real life,” Idea Group
Pulishing, 2007, pp. 385-426.
34
Marian Jureczko, Lech Madeyski
Politechniki Wrocławskiej, 2010, pp. 69-81. Applications-ICCSA 2012. Springer, 2012, pp.
[17] M. Jureczko and L. Madeyski, “Predykcja de- 234-247.
fektów na podstawie metryk oprogramowania [31] L. Madeyski and M. Jureczko, “Which
- identyfikacja klas projektów,” in Proceedings Process Metrics Can Significantly Imof
the Krajowa Konferencja Inżynierii Opro- prove Defect Prediction Models? An Emgramowania
(KKIO 2010). PWNT, 2010, pp. pirical Study,” Software Quality Journal,
185-192. Vol. 23, No. 3, 2015, pp. 393-422. [Online].
[18] Y. Liu, T. M. Khoshgoftaar, and N. Seliya, “Evo- http://dx.doi.org/10.1007/s11219-014-9241-7
lutionary optimization of software quality mod- [32] M. Jureczko and J. Magott, “QualitySpy:
eling with multiple repositories,” Software Engi- a framework for monitoring software developneering,
IEEE Transactions on, Vol. 36, No. 6, ment processes,” Journal of Theoretical and Ap2010,
pp. 852-864. plied Computer Science, Vol. 6, No. 1, 2012, pp.
[19] Z. He, F. Shu, Y. Yang, M. Li, and Q. Wang, “An 35-45.
investigation on the feasibility of cross-project [33] E. J. Weyuker, T. J. Ostrand, and R. M. Bell,
defect prediction,” Automated Software Engi- “Comparing the effectiveness of several modelneering,
Vol. 19, No. 2, 2012, pp. 167-199. ing methods for fault prediction,” Empirical
[20] S. R. Chidamber and C. F. Kemerer, “A metrics Software Engineering, Vol. 15, No. 3, 2010, pp.
suite for object oriented design,” IEEE Trans- 277-295.
actions on Software Engineering, Vol. 20, No. 6, [34] L. Madeyski, Test-driven development: An em1994,
pp. 476-493. pirical evaluation of agile practice. Springer,
[21] B. H. Sellers, Object-Oriented Metrics. Measures 2010.
of Complexity. Prentice Hall, 1996. [35] S. H. Kan, Metrics and models in software quality
[22] R. Martin, “OO design quality metrics,” An anal- engineering. Addison-Wesley Longman Publishysis
of dependencies, 1994. ing Co., Inc., 2002.
[23] J. Bansiya and C. G. Davis, “A hierarchical [36] M. Fischer, M. Pinzger, and H. Gall, “Populating
model for object-oriented design quality assess- a release history database from version control
ment,” IEEE Transactions on Software Engi- and bug tracking systems,” in International Conneering,
Vol. 28, No. 1, 2002, pp. 4-17. ference on Software Maintenance, 2003. ICSM
[24] M.-H. Tang, M.-H. Kao, and M.-H. Chen, “An 2003. Proceedings. IEEE, 2003, pp. 23-32.
empirical study on object-oriented metrics,” in [37] T. Zimmermann, R. Premraj, and A. Zeller,
Sixth International Software Metrics Symposium, “Predicting defects for eclipse,” in International
1999. Proceedings. IEEE, 1999, pp. 242-249. Workshop on Predictor Models in Software Engi[25]
T. J. McCabe, “A complexity measure,” IEEE neering, 2007. PROMISE'07: ICSE Workshops
Transactions on Software Engineering, No. 4, 2007. IEEE, 2007, pp. 9-9.
1976, pp. 308-320. [38] M. D'Ambros, A. Bacchelli, and M. Lanza, “On
[26] L. Madeyski, reproducer: Reproduce Statis- the impact of design flaws on software defects,”
tical Analyses and Meta-Analyses, 2015, R in Quality Software (QSIC), 2010 10th Internapackage.
[Online]. http://CRAN.R-project.org/ tional Conference on. IEEE, 2010, pp. 23-31.
/package=reproducer [39] M. D'Ambros, M. Lanza, and R. Robbes, “An
[27] L. Madeyski and B. A. Kitchenham, “Re- extensive comparison of bug prediction approducible
Research - What, Why and proaches,” in 7th IEEE Working Conference
How,” Wroclaw University of Technology, PRE on Mining Software Repositories (MSR), 2010.
W08/2015/P-020, 2015. IEEE, 2010, pp. 31-41.
[28] L. Madeyski, B. A. Kitchenham, and S. L. [40] A. Bacchelli, M. D'Ambros, and M. Lanza, “Are
Pfleeger, “Why Reproducible Research is Ben- popular classes more defect prone?” in Funeficial
for Security Research,” (under review), damental Approaches to Software Engineering.
2015. Springer, 2010, pp. 59-73.
[29] J. K. Chhabra and V. Gupta, “A survey of dy- [41] G. Antoniol, K. Ayari, M. Di Penta, F. Khomh,
namic software metrics,” Journal of computer and Y.-G. Guéhéneuc, “Is it a bug or an enhancescience
and technology, Vol. 25, No. 5, 2010, pp. ment?: a text-based approach to classify change
1016-1029. requests,” in Proceedings of the 2008 conference
[30] S. Misra, M. Koyuncu, M. Crasso, C. Mateos, of the center for advanced studies on collaborative
and A. Zunino, “A suite of cognitive complex- research: meeting of minds. ACM, 2008, p. 23.
ity metrics,” in Computational Science and Its [42] T. J. Ostrand, E. J. Weyuker, and R. M. Bell,
Cross-Project Defect Prediction With Respect To Code Ownership Model: An Empirical Study
35
“Where the bugs are,” in ACM SIGSOFT Software
Engineering Notes, Vol. 29, No. 4. ACM,
2004, pp. 86-96.
[43] V. R. Basili, L. C. Briand, and W. L. Melo,
“A validation of object-oriented design metrics
as quality indicators,” IEEE Transactions on
Software Engineering, Vol. 22, No. 10, 1996, pp.
751-761.
[44] F. Brito e Abreu and W. Melo, “Evaluating the
impact of object-oriented design on software
quality,” in Proceedings of the 3rd International
Software Metrics Symposium, 1996. IEEE, 1996,
pp. 90-99.
[45] W. L. Melo, L. Briand, and V. R. Basili, “Measuring
the impact of reuse on quality and productivity
in object-oriented systems,” 1998.
[46] K. Aggarwal, Y. Singh, A. Kaur, and R. Malhotra,
“Empirical study of object-oriented metrics,”
Journal of Object Technology, Vol. 5, No. 8, 2006,
pp. 149-173.
[47] P. Martenka and B. Walter, “Hierarchical
model for evaluating software design quality,”
e-Informatica Software Engineering Journal,
Vol. 4, No. 1, 2010, pp. 21-30.